{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Spam Filter with Naive Bayes\n",
    "## Introduction\n",
    "The aim in this project is to build a spam filter software, using the multinomial Naive Bayes Algorithm specifically. In essense, the filter would be able to categorise a new message as a spam or not. The standard for succes of this goal would be to developing a filter that is at least 80% accurate.\n",
    "\n",
    "This process requires a data set to build the algorithm for both training and testing purposes. Consequently, a data set from [UCI](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection) would be used. This data set contains 5572 categorised messages and was prepared by Tiago A. Almeida and José María Gómez Hidalgo, and is well sourced for the purpose of our project.\n",
    "\n",
    "The objectives for the project are as follows:\n",
    " * Train the filter software with the training data\n",
    " * Test the efficiency of the filter software using the test data\n",
    " \n",
    "\n",
    "## Initial EDA\n",
    "We begin the project by carrying out an initial exploratory data analytics (EDA) of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.593683\n",
       "spam    13.406317\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell  # Configure jupyther to displays...\n",
    "InteractiveShell.ast_node_interactivity = \"all\"             # multiple outputs at the same time.\n",
    "\n",
    "# import data-set\n",
    "sms_spam = pd.read_csv('Downloads/SMSSpamCollection', sep='\\t', header=None, names=['Label', 'SMS'])\n",
    "\n",
    "# General EDA\n",
    "sms_spam.head()\n",
    "sms_spam.tail()\n",
    "sms_spam.describe()\n",
    "sms_spam.shape\n",
    "sms_spam['Label'].unique()\n",
    "sms_spam['Label'].value_counts()\n",
    "sms_spam['Label'].value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the 'ham' label is used to categorise non-spam messages , while 'spam' is used for spam messages. Since both 'count' and shape show the same value then there are no instances of np.nans. Finally, among these mesaages, about 87% are non-spam messags while about 13% are spam messages. This sample looks representative, since in practice most messages that people receive are ham.\n",
    "\n",
    "## Data Split\n",
    "Next, we proceed to split our data for training and testing purposes and we have decided to apply a 80-20 split, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage distribution of training data:\n",
      " ham     86.53803\n",
      "spam    13.46197\n",
      "Name: Label, dtype: float64\n",
      "\n",
      "\n",
      "Percentage distribution of testing_data:\n",
      " ham     86.816143\n",
      "spam    13.183857\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Randomise the dataset\n",
    "randomised_sample = sms_spam.sample(frac = 1, random_state = 1)\n",
    "\n",
    "# Split the data set into training and testing data\n",
    "training_data = randomised_sample.iloc[:(round(0.8 * 5572)-1), :] # 80% for training data\n",
    "testing_data = randomised_sample.iloc[(round(0.8 * 5572)-1):, :]  # 20% of testing data\n",
    "\n",
    "# Reset index on both samples\n",
    "training_data.reset_index(drop = True, inplace = True)\n",
    "testing_data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Check data split \n",
    "print('Percentage distribution of training data:\\n',training_data['Label'].value_counts(normalize = True)*100)\n",
    "print('\\n')\n",
    "print('Percentage distribution of testing_data:\\n',testing_data['Label'].value_counts(normalize = True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we split the data into training and testing data, 80% and 20% respectively. Notice that before this split, we randomised the full initial data sample. We did this to facilitate consistency in the proportion of ham and spam (about 87% and 13%) messages post-split, which could have been introduced with the original sorting order of our data sample. \n",
    "\n",
    "## Implement Training Data on Software Filter\n",
    "### Clean Data \n",
    "Now we can implement the training data to train the multinomial Naive Bayes algorithm based software filter. The first thing to do is clean the data by removing all punctuation marks and rendering all messages in lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                         Yep, by the pretty sculpture\n",
       "1        Yes, princess. Are you going to make me moan?\n",
       "2                           Welp apparently he retired\n",
       "3                                              Havent.\n",
       "4    I forgot 2 ask ü all smth.. There's a card on ...\n",
       "Name: SMS, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                         yep  by the pretty sculpture\n",
       "1        yes  princess  are you going to make me moan \n",
       "2                           welp apparently he retired\n",
       "3                                              havent \n",
       "4    i forgot 2 ask ü all smth   there s a card on ...\n",
       "Name: SMS, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the training data set \n",
    "clean_sms = training_data['SMS'].str.replace('\\W', ' ').str.lower() \n",
    "\n",
    "print('Before cleaning:')\n",
    "training_data['SMS'].head()\n",
    "\n",
    "print('After cleaning:')\n",
    "clean_sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, puctuation marks have been removed and lowercase words have been rendered.\n",
    "\n",
    "### Transform Data \n",
    "In order to implement the Naive Bayes Algorithm on messages data, this data first needs to be transformed into a numeric format, which supports to calculations of the algorithm but still represents the message data. This format is highlighted below: \n",
    "\n",
    "![Show picture](https://dq-content.s3.amazonaws.com/433/cpgp_dataset_3.png)\n",
    "\n",
    "Steps would now be taken to transform our training data.\n",
    "\n",
    "First, we extract the vocabulary of our training set, which is effectively all the unique words from our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 unique words are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'jetton',\n",
       " 'bros',\n",
       " 'results',\n",
       " 'jul',\n",
       " 'dose',\n",
       " '0800',\n",
       " 'lovers',\n",
       " 'wait',\n",
       " 'diff']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words from out our training data: 7783\n"
     ]
    }
   ],
   "source": [
    "clean_sms = clean_sms.str.strip().str.split(' ').copy()\n",
    "\n",
    "all_words_all_messages = []                  # Create container that would contains...\n",
    "                                             # all words from all messages\n",
    "    \n",
    "# Define a function to extract all the word from all messages \n",
    "def extract_words(lst):\n",
    "    for each in lst:\n",
    "        all_words_all_messages.append(each)\n",
    "    return                                   # return nothing\n",
    "        \n",
    "# Run function to extract all the word from all messages\n",
    "psedo_output_file = clean_sms.apply(extract_words) \n",
    "\n",
    "# Create Vocabulary by extracting unique words from 'all_words_all_messages'\n",
    "vocabulary = set(all_words_all_messages)\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "# Check results\n",
    "print('First 10 unique words are:')\n",
    "vocabulary[:10]\n",
    "print('Total unique words from out our training data:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully extracted the unique set of words from all messages in our training data in the variable 'vocabulary'. Notice from the output above that an empty string has been characterised as a word. This would be addressed after the transformation is complete.\n",
    "\n",
    "Now we proceed to transform the trianing data into the format illustrated in the image above, which is a numeric format that supports to calculations of the algorithm but still represents the message data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ad6a2a3c00ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Categorise the frequency of each word in every message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m word_counts_per_sms = {unique_word: [0] * \n\u001b[0;32m----> 3\u001b[0;31m                        len(clean_sms) for unique_word in vocabulary_list} # Create a container dictionary with...\n\u001b[0m\u001b[1;32m      4\u001b[0m                                                                           \u001b[0;31m# the correct dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_sms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Categorise the frequency of each word in every message\n",
    "word_counts_per_sms = {unique_word: [0] * \n",
    "                       len(clean_sms) for unique_word in vocabulary_list} # Create a container dictionary with...\n",
    "                                                                          # the correct dimensions\n",
    "for index, sms in enumerate(clean_sms):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1\n",
    "transform_df = pd.DataFrame(word_counts_per_sms)                          # Create transformation...\n",
    "                                                                          # dataframe \n",
    "\n",
    "# Extend the training data by its transformed format\n",
    "transform_training_data = pd.concat([training_data, transform_df], axis = 1)\n",
    "\n",
    "# Validate the transformed dataframe\n",
    "print('Total word from our transformed words frequency table:'\n",
    "      , transform_df.sum().sum())\n",
    "print('Unique words from our training data:'\n",
    "      , len(all_words_all_messages))\n",
    "\n",
    "# Remove the empty strings that have been characterised as words\n",
    "transform_training_data.drop(columns  = '', inplace = True) \n",
    "transform_training_data.head()\n",
    "\n",
    "# Remove the empty string as a word from our set of unique words\n",
    "while '' in vocabulary_list:\n",
    "    vocabulary_list.remove('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have successfully created a numeric format which represents messages in our training data and also supports calculations for our multinomial Naive Bayes Algorithm.\n",
    "\n",
    "It was also important to check the validity of the transformation. We confirmed that the sum of the frequency from our transformation table is equal to the number of all words extracted from all messages ('all_words_all_messages'). \n",
    "\n",
    "\n",
    "### Calculate constants\n",
    "We're now done with cleaning the training set, and we can begin creating the spam filter. The Naive Bayes algorithm will need to answer these two probability questions to be able to classify new messages:\n",
    "$$P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)$$\n",
    "$$P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)$$\n",
    "\n",
    "Also, to calculate P(wi|Spam) and P(wi|Ham) inside the formulas above, we'll need to use these equations:\n",
    "$$P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}$$\n",
    "$$P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}$$\n",
    "\n",
    "Some of the terms in the four equations above will have the same value for every new message. We can calculate the value of these terms once and avoid doing the computations again when a new messages comes in. Below, we'll use our training set to calculate:\n",
    "* P(Spam) and P(Ham)\n",
    "* NSpam, NHam, NVocabulary\n",
    "\n",
    "We'll also use Laplace smoothing and set $\\alpha = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the constants of the Naive Bayes Algorithm\n",
    "alpha = 1 \n",
    "n_vocabulary = len(vocabulary_list) \n",
    "\n",
    "n_of_words_in_spam = transform_training_data[\n",
    "                                      transform_training_data['Label']\n",
    "                                      == \n",
    "                                      'spam'\n",
    "                                     ].iloc[:,2:].sum().sum()\n",
    "\n",
    "n_of_words_in_ham = transform_training_data[\n",
    "                                     transform_training_data['Label']\n",
    "                                     == \n",
    "                                     'ham'\n",
    "                                    ].iloc[:,2:].sum().sum()\n",
    "\n",
    "p_spam = ((training_data['Label'] == 'spam').sum()\n",
    "          / \n",
    "          len(training_data))\n",
    "\n",
    "p_ham = ((training_data['Label'] == 'ham').sum()\n",
    "         / \n",
    "         len(training_data))\n",
    "\n",
    "word_count_given_spam_freq_table = transform_training_data[\n",
    "                                                       transform_training_data['Label'] \n",
    "                                                       == \n",
    "                                                       'spam'\n",
    "                                                      ].iloc[:,2:].sum()\n",
    "\n",
    "word_count_given_ham_freq_table = transform_training_data[\n",
    "                                                      transform_training_data['Label']\n",
    "                                                      ==\n",
    "                                                      'ham'\n",
    "                                                     ].iloc[:,2:].sum()\n",
    "print('Total number of unique words in our vocabulary:', n_vocabulary)\n",
    "print('Total number of words in spam messages:', n_of_words_in_spam)\n",
    "print('Total number of words in ham messages:', n_of_words_in_ham)\n",
    "print('Probability of a spam message:', p_spam)\n",
    "print('Probability of a ham message:', p_ham)\n",
    "print('\\n')\n",
    "print('Unique word count in all spam messages')\n",
    "word_count_given_spam_freq_table.head()\n",
    "print('Unique word count in all ham messages')\n",
    "word_count_given_ham_freq_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate parameters\n",
    "Above, we have calculated the constants. Now we can move on with calculating the parameters $P(w_i|Spam)$ and $P(w_i|Ham)$. Each parameter will thus be a conditional probability value associated with each word in the vocabulary.\n",
    "The parameters are calculated using the formulas:\n",
    "$$P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}$$\n",
    "$$P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}$$\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create container for parameters\n",
    "p_word_given_spam_dic = {each_word : [0] for each_word in vocabulary_list}\n",
    "p_word_given_ham_dic = {each_word : [0] for each_word in vocabulary_list}\n",
    "\n",
    "# Calculate and store parameters\n",
    "for each_word in vocabulary_list:\n",
    "    p_word_given_spam_dic[each_word] = ((word_count_given_spam_freq_table[each_word] + alpha) \n",
    "                                        / \n",
    "                                        (n_of_words_in_spam + (alpha * n_vocabulary)))\n",
    "\n",
    "    p_word_given_ham_dic[each_word] = ((word_count_given_ham_freq_table[each_word] + alpha) \n",
    "                                       /\n",
    "                                       (n_of_words_in_ham + (alpha * n_vocabulary)))\n",
    "    \n",
    "for each in vocabulary_list[:5]:\n",
    "    print('Given spam,the probability of',each, p_word_given_spam_dic[each])\n",
    "print('\\n')\n",
    "for each in vocabulary_list[:5]:\n",
    "    print('Given non_spam,the probability of', each, p_word_given_ham_dic[each])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we calculated all the parameters for out multinomial Naive Bayes Algorithm.  We ended up with two dictionaries with  calculated probabilities for each unique word, from our training data, occuring in either a spam or non spam message. \n",
    "\n",
    "With these constants and parameters calculated, we can proceed to define our functions that would categorise messages as either spam or non-spam messages based on these parameters calculated above, using the multinomial Naive Bayes Algorithm.\n",
    "\n",
    "## Creating the Classifying Function\n",
    "The spam filter can be understood as a function that:\n",
    "* Takes in as input a new message (w1, w2, ..., wn).\n",
    "* Calculates P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn).\n",
    "* Compares the values of P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn), and:\n",
    " * If P(Ham|w1, w2, ..., wn) > P(Spam|w1, w2, ..., wn), then the message is classified as ham.\n",
    " * If P(Ham|w1, w2, ..., wn) < P(Spam|w1, w2, ..., wn), then the message is classified as spam.\n",
    " * If P(Ham|w1, w2, ..., wn) = P(Spam|w1, w2, ..., wn), then the algorithm may request human help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define function to classify messages as spam or non-spam\n",
    "def classify_test_set(message):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    for each in message:\n",
    "        if each in p_word_given_spam_dic:\n",
    "            p_spam_given_message *= p_word_given_spam_dic[each]\n",
    "        if each in p_word_given_ham_dic:\n",
    "            p_ham_given_message *= p_word_given_ham_dic[each]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'\n",
    "    \n",
    "# Test software filter with testing data    \n",
    "testing_data['predicted'] = testing_data['SMS'].apply(classify_test_set)\n",
    "testing_data.head()\n",
    "\n",
    "correct = 0\n",
    "total = len(testing_data)\n",
    "\n",
    "for idx, data in testing_data.iterrows():\n",
    "    if data['Label'] == data['predicted']:\n",
    "        correct += 1\n",
    "        \n",
    "accuracy = correct/total\n",
    "print('Correct:',correct)\n",
    "print('Incorrect:', total - correct)\n",
    "print('From the test run with the testing data, the software filter is',accuracy * 100, 'percent accurate.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Classifying Function\n",
    "Above, we defined a function to classify new messages based on the multinomial Naive Bayes Algorithm. We tested the accuracy of the software filter to be 98.74%.\n",
    "\n",
    "## Conclusion\n",
    "We tested the accuracy of our software filter based on out testing data and found that the filter works well, with a 98.74% accuracy. Our initial goal was an accuracy of over 80%, and we managed to do way better than that.\n",
    "\n",
    "Next steps include:\n",
    "* Analyze the 14 messages that were classified incorrectly and try to figure out why the algorithm classified them incorrectly\n",
    "* Make the filtering process more complex by making the algorithm sensitive to letter case\n",
    "\n",
    "\n",
    "id: dismfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
